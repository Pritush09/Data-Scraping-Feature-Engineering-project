{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████████████████▏                                                   | 36/100 [01:57<03:41,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████████▋                                         | 49/100 [02:34<02:31,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [05:56<00:00,  3.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from Data_Extraction import data_extracter\n",
    "\n",
    "data = pd.read_csv('C:\\\\Users\\\\mynam\\\\Test Project\\\\Input.xlsx - Sheet1.csv')\n",
    "# provide the path for the input csv file so the further processess can be caried out\n",
    "\n",
    "# extracting the content by using the data_extracter funciton in the helper.py file \n",
    "datafa = data_extracter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data_Extraction import read_custom_stopwords\n",
    "stopword_auditors = read_custom_stopwords('C:\\\\Users\\\\mynam\\\\Test Project\\\\StopWords\\\\StopWords_Auditor.txt')\n",
    "stopword_Currency = read_custom_stopwords('C:\\\\Users\\\\mynam\\\\Test Project\\\\StopWords\\\\StopWords_Currencies.txt')\n",
    "StopWords_DatesandNumbers = read_custom_stopwords('C:\\\\Users\\\\mynam\\\\Test Project\\\\StopWords\\\\StopWords_DatesandNumbers.txt')\n",
    "stopword_generic = read_custom_stopwords('C:\\\\Users\\\\mynam\\\\Test Project\\\\StopWords\\\\StopWords_Generic.txt')\n",
    "StopWords_GenericLong = read_custom_stopwords('C:\\\\Users\\\\mynam\\\\Test Project\\\\StopWords\\\\StopWords_GenericLong.txt')\n",
    "StopWords_Geographic = read_custom_stopwords('C:\\\\Users\\\\mynam\\\\Test Project\\\\StopWords\\\\StopWords_Geographic.txt')\n",
    "stopword_Names = read_custom_stopwords('C:\\\\Users\\\\mynam\\\\Test Project\\\\StopWords\\\\StopWords_Names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_to_use_combined = list()\n",
    "for element in stopword_Currency:\n",
    "    lsitele = element.split(\"|\")[0]\n",
    "    StopWords_to_use_combined.append(lsitele)\n",
    "for ele in stopword_Names:\n",
    "    StopWords_to_use_combined.append(ele)\n",
    "for ele in StopWords_Geographic:\n",
    "    StopWords_to_use_combined.append(ele)\n",
    "for ele in StopWords_GenericLong:\n",
    "    StopWords_to_use_combined.append(ele)\n",
    "for ele in stopword_generic:\n",
    "    StopWords_to_use_combined.append(ele)\n",
    "for ele in StopWords_DatesandNumbers:\n",
    "    StopWords_to_use_combined.append(ele)\n",
    "for ele in stopword_auditors:\n",
    "    StopWords_to_use_combined.append(ele)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_to_use_combined = [stopwords.lower() for stopwords in StopWords_to_use_combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafa.content = datafa.content.apply(lambda x: remove_stopwords(x,StopWords_to_use_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_custom_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        stopwords_list = [line.strip() for line in file.readlines()]\n",
    "    return set(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativewords = read_custom_words('C:\\\\Users\\\\mynam\\\\Test Project\\\\MasterDictionary\\\\negative-words.txt')\n",
    "positivewords = read_custom_words('C:\\\\Users\\\\mynam\\\\Test Project\\\\MasterDictionary\\\\positive-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\mynam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mynam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk as nlt\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "nlt.download('cmudict')  \n",
    "\n",
    "def counter_pos(txt):\n",
    "    count = 0\n",
    "    words = txt.split()\n",
    "    for word in words:\n",
    "        if word.lower() in positivewords:\n",
    "            count = count + 1\n",
    "    return(count)\n",
    "\n",
    "def counter_neg(txt):\n",
    "    count = 0\n",
    "    words = txt.split()\n",
    "    for word in words:\n",
    "        if word.lower() in negativewords:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def count_complex_words(text):\n",
    "    \"\"\"Function to count the number of complex word using the CMU Pronouncing Dictionary\"\"\"\n",
    "\n",
    "    def count_syllables(word):\n",
    "        vowels = \"aeiouyAEIOUY\"\n",
    "        count = 0\n",
    "        is_vowel = False\n",
    "\n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                if not is_vowel:\n",
    "                    count += 1\n",
    "                    is_vowel = True\n",
    "            else:\n",
    "                is_vowel = False\n",
    "\n",
    "        return max(1, count)  # Ensure at least one syllable for non-empty words\n",
    "\n",
    "\n",
    "    d = cmudict.dict()\n",
    "    words = nlt.word_tokenize(text)\n",
    "\n",
    "    # Count the number of words with more than two syllables\n",
    "    complex_word_count = sum(1 for word in words if count_syllables(word) > 2)\n",
    "\n",
    "    return complex_word_count\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nlt.download('stopwords')\n",
    "\n",
    "def count_cleaned_words(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nlt.word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    words = [word.strip(string.punctuation) for word in words]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Count the number of cleaned words\n",
    "    cleaned_word_count = len(words)\n",
    "\n",
    "    return cleaned_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafa['POSITIVE SCORE'] = datafa.content.apply(counter_pos)\n",
    "datafa['NEGATIVE SCORE'] = datafa.content.apply(counter_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafa.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafa[\"Total number of words\"] = datafa.content.apply(lambda x: len(nlt.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafa[\"Total number of Sentences\"] = datafa.content.apply(lambda x: len(nlt.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "datafa[\"POLARITY SCORE\"]  =(datafa['POSITIVE SCORE'] - datafa['NEGATIVE SCORE'])/((datafa['POSITIVE SCORE'] + datafa['NEGATIVE SCORE'])+0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "# Range is from 0 to +1\n",
    "datafa['SUBJECTIVE SCORE'] =(datafa['POSITIVE SCORE']+datafa['NEGATIVE SCORE'])/((datafa['Total number of words'])+ 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafa['COMPLEX WORD COUNT'] = datafa.content.apply(count_complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG SENTENCE LENGTH =  the number of words / the number of sentences\n",
    "datafa['AVG SENTENCE LENGTH'] =datafa['Total number of words']/ datafa['Total number of Sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERCENTAGE OF COMPLEX WORDS = number of complex words / the number of words \n",
    "datafa['PERCENTAGE OF COMPLEX WORDS']=datafa['COMPLEX WORD COUNT']/datafa['Total number of words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "datafa['FOG INDEX']=0.4*(datafa['AVG SENTENCE LENGTH']+datafa['PERCENTAGE OF COMPLEX WORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG NUMBER OF WORDS PER SENTENCE = the total number of words / the total number of sentences\n",
    "datafa['AVG NUMBER OF WORDS PER SENTENCE'] = datafa['Total number of words']/ datafa['Total number of Sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafa['WORD COUNT']=datafa.content.apply(count_cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    vowels = \"aeiouyAEIOUY\"\n",
    "    count = 0\n",
    "    is_vowel = False\n",
    "\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            if not is_vowel:\n",
    "                count += 1\n",
    "                is_vowel = True\n",
    "        else:\n",
    "            is_vowel = False\n",
    "\n",
    "    return max(1, count)  # Ensure at least one syllable for non-empty words\n",
    "\n",
    "def count_syllables_in_text(words):\n",
    "    syllable_count = sum(count_syllables(word) for word in words)\n",
    "    return syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def total_syllables(text):\n",
    "    words = nlt.word_tokenize(text)\n",
    "    words = np.array([word.strip(string.punctuation) for word in words])\n",
    "\n",
    "    # Vectorized check for strings ending with 'es' or 'ed'  \n",
    "    #  https://stackoverflow.com/questions/44494562/numpy-vectorized-check-if-strings-in-array-end-with-strings-in-another-array\n",
    "    ends_with_es_ed = np.core.defchararray.endswith(words[:,None], ['es', 'ed'])\n",
    "    \n",
    "    list_of_elements_to_delete = list()\n",
    "    for i in range(len(words)):\n",
    "        if ends_with_es_ed[i].sum() == 1:\n",
    "            list_of_elements_to_delete.append(i)\n",
    "\n",
    "    np_words = np.delete(words, list_of_elements_to_delete)\n",
    "            \n",
    "    return count_syllables_in_text(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYLLABLE PER WORD = total syllable / total no. of words \n",
    "\n",
    "datafa['SYLLABLE PER WORD']=(datafa.content.apply(total_syllables)/datafa['Total number of words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of personal pronouns in the text is: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def count_personal_pronouns(text):\n",
    "    # Define a regular expression pattern for matching personal pronouns\n",
    "    pronoun_pattern = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "\n",
    "    # Use re.findall to find all matches in the text\n",
    "    pronoun_matches = re.findall(pronoun_pattern, text, flags=re.IGNORECASE)\n",
    "    # print(pronoun_matches)\n",
    "\n",
    "    # Exclude instances where \"US\" is part of a word\n",
    "    pronoun_count = sum(1 for match in pronoun_matches if match != 'US')\n",
    "\n",
    "    return pronoun_count\n",
    "\n",
    "# Example usage:\n",
    "text = \"I went to the store. We bought groceries. My car is parked outside. Ours is a beautiful garden. The US economy is strong. us\"\n",
    "result = count_personal_pronouns(text)\n",
    "print(f'The number of personal pronouns in the text is: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERSONAL PRONOUNS \n",
    "datafa['PERSONAL PRONOUNS']=datafa.content.apply(count_personal_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(text):\n",
    "    words = nlt.word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    words = [word.strip(string.punctuation) for word in words]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    combined_length_of_each_word = sum(len(word) for word in words)\n",
    "    return (combined_length_of_each_word/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG WORD LENGTH = \n",
    "datafa['AVG WORD LENGTH']=datafa.content.apply(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdf = datafa.drop(columns=['Total number of Sentences','Total number of words','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate for the csv file\n",
    "csv_file_path = 'Output.csv'\n",
    "outputdf.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
